{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import yaml\n",
    "\n",
    "\n",
    "def load_config(filename: str) -> object:\n",
    "    file = open(filename).read()\n",
    "    return yaml.safe_load(file)"
   ],
   "id": "255aea07e253c63e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def load_template(filename: str) -> str:\n",
    "    file = open(filename, 'r').read()\n",
    "    return file"
   ],
   "id": "35a01221e6c6b6f7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from jinja2 import Template\n",
    "from openai import OpenAI\n",
    "from dqa.settings import OPENAI_API_KEY, OPENAI_BASE_URL, MAX_TOKENS_OUTPUT, PROMPT_TEMPLATE\n",
    "\n",
    "\n",
    "class Expert:\n",
    "    client = None\n",
    "\n",
    "    def __init__(self, model, base_url=None, api_key=None, prompt_template=None):\n",
    "        self.model = model\n",
    "        self.base_url = base_url or OPENAI_BASE_URL\n",
    "        self.api_key = api_key or OPENAI_API_KEY\n",
    "        self.prompt_template = prompt_template or PROMPT_TEMPLATE\n",
    "\n",
    "        if self.client is None:\n",
    "            self.init_client()\n",
    "\n",
    "    def init_client(self):\n",
    "        self.client = OpenAI(api_key=self.api_key, base_url=self.base_url)\n",
    "\n",
    "    def rate_sample(self, sample: str) -> int | None:\n",
    "        prompt = Template(self.prompt_template).render(context=sample)\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model, messages=messages,\n",
    "            temperature=0, n=1, max_tokens=MAX_TOKENS_OUTPUT\n",
    "        )\n",
    "\n",
    "        # Extract the response text and parse it as an integer rating from 1 to 5\n",
    "        response_text = response.choices[0].message.content\n",
    "        try:\n",
    "            rating = int(''.join(filter(str.isdigit, response_text)))\n",
    "        except ValueError:\n",
    "            print(\"Invalid rating value in response: {}\".format(response_text))\n",
    "            return None\n",
    "\n",
    "        # Ensure the rating is within the valid range [1, 5]\n",
    "        if rating < 1: rating = 1\n",
    "        if rating > 5: rating = 5\n",
    "\n",
    "        return rating"
   ],
   "id": "4ef84fcab26ca504",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "import concurrent.futures\n",
    "from datasets import load_dataset\n",
    "import tiktoken\n",
    "\n",
    "config = load_config('experts.yml')\n",
    "template = load_template('prompt_template.txt')\n",
    "experts = [Expert(**expert, prompt_template=template) for expert in config['experts']]\n",
    "encoding = tiktoken.get_encoding(\"gpt2\")  # Load the Tiktoken encoding model for counting tokens\n",
    "\n",
    "\n",
    "# Processing function, due to limitation of concurrent requests in python\n",
    "def process_model(expert: Expert, data: str) -> int:\n",
    "    return expert.rate_sample(data)\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"IlyaGusev/saiga_scored\", split=\"train[:10]\")  # Replace with your desired dataset name\n",
    "\n",
    "# Get the data to rate from the dataset\n",
    "data_to_rate = [json.dumps(sample[\"messages\"], ensure_ascii=False) for sample in dataset]"
   ],
   "id": "9420ea9de5dd79c9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import concurrent.futures\n",
    "from dqa.settings import MAX_TOKENS_INPUT\n",
    "\n",
    "\n",
    "def rate_sample(sample: str, experts: list, max_workers: int = 4):\n",
    "    # Count the number of tokens in the sample\n",
    "    num_tokens = len(encoding.encode(sample))\n",
    "    if num_tokens > MAX_TOKENS_INPUT:\n",
    "        print(f\"Skipping sample: Size exceeds limit ({num_tokens} tokens)\")\n",
    "        return None\n",
    "\n",
    "    print(f\"Processing sample: {sample}\")\n",
    "\n",
    "    # Define a function to be executed by each worker\n",
    "    def rate_expert(expert):\n",
    "        return expert.model, process_model(expert, sample)\n",
    "\n",
    "    # Create a thread pool with the specified number of workers\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        ratings = list(executor.map(rate_expert, experts))\n",
    "\n",
    "    # Convert the list of tuples into a dictionary\n",
    "    ratings_by_expert = {model: rating for model, rating in ratings}\n",
    "    print(\"Ratings:\", ratings_by_expert)\n",
    "\n",
    "    return ratings_by_expert\n",
    "\n",
    "\n",
    "# max_workers = 1\n",
    "max_workers = len(experts)\n",
    "\n",
    "# Example usage\n",
    "ratings_by_experts = []\n",
    "for i, data in enumerate(data_to_rate):\n",
    "    ratings_by_expert = rate_sample(data, experts, max_workers=max_workers)\n",
    "    ratings_by_experts.append(ratings_by_expert)"
   ],
   "id": "733eb4a40b245364",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import statistics\n",
    "\n",
    "\n",
    "def classify_rating(ratings_by_experts: list, quorum: int):\n",
    "    classified_samples = []\n",
    "\n",
    "    for i, ratings in enumerate(ratings_by_experts):\n",
    "\n",
    "        # Skip empty ratings\n",
    "        if ratings is None:\n",
    "            continue\n",
    "\n",
    "        # Count the number of valid (non-None) ratings\n",
    "        valid_ratings = [rating for rating in ratings.values() if rating is not None]\n",
    "\n",
    "        # Check if the number of valid ratings meets or exceeds the quorum\n",
    "        majority_vote = True\n",
    "        if len(valid_ratings) < quorum:\n",
    "            majority_vote = False\n",
    "            print(f\"Sample {i + 1}: Not enough valid ratings. Skipping classification.\")\n",
    "\n",
    "        # Calculate the average rating\n",
    "        average_rating = statistics.mean(valid_ratings)\n",
    "        classification = \"Good\" if average_rating > 3.5 else \"Bad\"\n",
    "\n",
    "        # Check for a majority vote\n",
    "        print(f\"Sample {i + 1}: AVG Rating: {average_rating}, Class: {classification}\")\n",
    "        print(f\"Ratings: {json.dumps(valid_ratings)}\")\n",
    "        print(f\"Majority Vote Achieved: {majority_vote}\")\n",
    "        print(f\"\")\n",
    "\n",
    "        classified_samples.append({\n",
    "            \"sample_index\": i + 1,\n",
    "            \"average_rating\": average_rating,\n",
    "            \"classification\": classification,\n",
    "            \"majority_vote\": majority_vote\n",
    "        })\n",
    "\n",
    "    return classified_samples if classified_samples else None\n",
    "\n",
    "\n",
    "# Process each element concurrently using a pool of experts and take a quorum vote\n",
    "quorum = (len(experts) // 2) + 1  # (50%+1) calculate the minimum number of experts to get a majority vote\n",
    "\n",
    "# Calculate the average rating and classify it as \"bad\" or \"good\" for each sample\n",
    "results = classify_rating(ratings_by_experts, quorum)\n",
    "results"
   ],
   "id": "d0798be8b3dade52",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
