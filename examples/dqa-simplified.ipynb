{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-16T11:11:43.315867Z",
     "start_time": "2024-08-16T11:11:43.303072Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from os.path import join, dirname\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load settings from environments\n",
    "dotenv_path = join(dirname('.'), '.env')\n",
    "load_dotenv(dotenv_path, override=False)"
   ],
   "id": "9420ea9de5dd79c9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-16T11:11:43.829204Z",
     "start_time": "2024-08-16T11:11:43.317389Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dqa.utils import load_template, load_config\n",
    "from dqa.expert import Expert\n",
    "\n",
    "# Load configuration\n",
    "config = load_config('experts.yml')\n",
    "template = load_template('prompt_template.txt')\n",
    "\n",
    "# Init experts\n",
    "experts = [\n",
    "    Expert(\n",
    "        **expert,\n",
    "        prompt_template=expert.get('prompt_template', template)\n",
    "    )\n",
    "    for expert in config['experts']\n",
    "]"
   ],
   "id": "fe473b0e4e34a1d9",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-16T11:11:48.313401Z",
     "start_time": "2024-08-16T11:11:43.830185Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"IlyaGusev/saiga_scored\", split=\"train[:10]\")  # Replace with your desired dataset name"
   ],
   "id": "e263eb9d2074f330",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-16T11:13:01.375689Z",
     "start_time": "2024-08-16T11:12:26.649197Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "from dqa.expert import rate_sample\n",
    "\n",
    "# Example usage\n",
    "ratings_by_experts = []\n",
    "for data in dataset:\n",
    "    sample = json.dumps(data[\"messages\"], ensure_ascii=False)\n",
    "    ratings_by_expert = rate_sample(sample, experts, max_workers=len(experts))\n",
    "    ratings_by_experts.append(ratings_by_expert)\n",
    "\n",
    "ratings_by_experts"
   ],
   "id": "733eb4a40b245364",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model: google/gemma-2-9b-it - Invalid rating value in response: ::::::::\n",
      "Model: google/gemma-2-9b-it - Invalid rating value in response: :::::::\n",
      "Model: google/gemma-2-9b-it - Invalid rating value in response: :::::::\n",
      "Model: perplexity/llama-3-sonar-small-32k-online - Invalid rating value in response: I have read the sources carefully. Please go ahead\n",
      "Skipping sample: Size exceeds limit (10593 tokens)\n",
      "Model: google/gemma-2-9b-it - Invalid rating value in response: \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Skipping sample: Size exceeds limit (7171 tokens)\n",
      "Model: google/gemma-2-9b-it - Invalid rating value in response: :::::::\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'gpt-3.5-turbo': 5,\n",
       "  'anthropic/claude-3-haiku': 5,\n",
       "  'perplexity/llama-3-sonar-small-32k-online': 4,\n",
       "  'google/palm-2-chat-bison-32k': 4,\n",
       "  'google/gemma-2-9b-it': None},\n",
       " {'gpt-3.5-turbo': 5,\n",
       "  'anthropic/claude-3-haiku': 4,\n",
       "  'perplexity/llama-3-sonar-small-32k-online': 4,\n",
       "  'google/palm-2-chat-bison-32k': 4,\n",
       "  'google/gemma-2-9b-it': None},\n",
       " {'gpt-3.5-turbo': 5,\n",
       "  'anthropic/claude-3-haiku': 4,\n",
       "  'perplexity/llama-3-sonar-small-32k-online': 4,\n",
       "  'google/palm-2-chat-bison-32k': 4,\n",
       "  'google/gemma-2-9b-it': None},\n",
       " {'gpt-3.5-turbo': 5,\n",
       "  'anthropic/claude-3-haiku': 5,\n",
       "  'perplexity/llama-3-sonar-small-32k-online': 4,\n",
       "  'google/palm-2-chat-bison-32k': 4,\n",
       "  'google/gemma-2-9b-it': 4},\n",
       " {'gpt-3.5-turbo': 5,\n",
       "  'anthropic/claude-3-haiku': 5,\n",
       "  'perplexity/llama-3-sonar-small-32k-online': None,\n",
       "  'google/palm-2-chat-bison-32k': 4,\n",
       "  'google/gemma-2-9b-it': 4},\n",
       " None,\n",
       " {'gpt-3.5-turbo': 5,\n",
       "  'anthropic/claude-3-haiku': 5,\n",
       "  'perplexity/llama-3-sonar-small-32k-online': 4,\n",
       "  'google/palm-2-chat-bison-32k': 4,\n",
       "  'google/gemma-2-9b-it': 4},\n",
       " {'gpt-3.5-turbo': 5,\n",
       "  'anthropic/claude-3-haiku': 5,\n",
       "  'perplexity/llama-3-sonar-small-32k-online': 4,\n",
       "  'google/palm-2-chat-bison-32k': 5,\n",
       "  'google/gemma-2-9b-it': None},\n",
       " None,\n",
       " {'gpt-3.5-turbo': 5,\n",
       "  'anthropic/claude-3-haiku': 5,\n",
       "  'perplexity/llama-3-sonar-small-32k-online': 4,\n",
       "  'google/palm-2-chat-bison-32k': 4,\n",
       "  'google/gemma-2-9b-it': None}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-16T11:13:09.555870Z",
     "start_time": "2024-08-16T11:13:09.550298Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dqa.helpers import classify_rating\n",
    "\n",
    "# Process each element concurrently using a pool of experts and take a quorum vote\n",
    "quorum = (len(experts) // 2) + 1  # (50%+1) calculate the minimum number of experts to get a majority vote\n",
    "\n",
    "# Calculate the average rating and classify it as \"bad\" or \"good\" for each sample\n",
    "results = classify_rating(ratings_by_experts, quorum)\n",
    "\n",
    "results"
   ],
   "id": "d0798be8b3dade52",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1: AVG Rating: 4.5, Class: Good\n",
      "Ratings: [5, 5, 4, 4]\n",
      "Majority Vote Achieved: True\n",
      "\n",
      "Sample 2: AVG Rating: 4.25, Class: Good\n",
      "Ratings: [5, 4, 4, 4]\n",
      "Majority Vote Achieved: True\n",
      "\n",
      "Sample 3: AVG Rating: 4.25, Class: Good\n",
      "Ratings: [5, 4, 4, 4]\n",
      "Majority Vote Achieved: True\n",
      "\n",
      "Sample 4: AVG Rating: 4.4, Class: Good\n",
      "Ratings: [5, 5, 4, 4, 4]\n",
      "Majority Vote Achieved: True\n",
      "\n",
      "Sample 5: AVG Rating: 4.5, Class: Good\n",
      "Ratings: [5, 5, 4, 4]\n",
      "Majority Vote Achieved: True\n",
      "\n",
      "Sample 7: AVG Rating: 4.4, Class: Good\n",
      "Ratings: [5, 5, 4, 4, 4]\n",
      "Majority Vote Achieved: True\n",
      "\n",
      "Sample 8: AVG Rating: 4.75, Class: Good\n",
      "Ratings: [5, 5, 4, 5]\n",
      "Majority Vote Achieved: True\n",
      "\n",
      "Sample 10: AVG Rating: 4.5, Class: Good\n",
      "Ratings: [5, 5, 4, 4]\n",
      "Majority Vote Achieved: True\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'sample_index': 1,\n",
       "  'average_rating': 4.5,\n",
       "  'classification': 'Good',\n",
       "  'majority_vote': True},\n",
       " {'sample_index': 2,\n",
       "  'average_rating': 4.25,\n",
       "  'classification': 'Good',\n",
       "  'majority_vote': True},\n",
       " {'sample_index': 3,\n",
       "  'average_rating': 4.25,\n",
       "  'classification': 'Good',\n",
       "  'majority_vote': True},\n",
       " {'sample_index': 4,\n",
       "  'average_rating': 4.4,\n",
       "  'classification': 'Good',\n",
       "  'majority_vote': True},\n",
       " {'sample_index': 5,\n",
       "  'average_rating': 4.5,\n",
       "  'classification': 'Good',\n",
       "  'majority_vote': True},\n",
       " {'sample_index': 7,\n",
       "  'average_rating': 4.4,\n",
       "  'classification': 'Good',\n",
       "  'majority_vote': True},\n",
       " {'sample_index': 8,\n",
       "  'average_rating': 4.75,\n",
       "  'classification': 'Good',\n",
       "  'majority_vote': True},\n",
       " {'sample_index': 10,\n",
       "  'average_rating': 4.5,\n",
       "  'classification': 'Good',\n",
       "  'majority_vote': True}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
